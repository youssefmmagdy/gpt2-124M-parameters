# üß† Mini GPT-2 Transformer from Scratch

This repository contains a PyTorch implementation of a GPT-2-style Transformer model, built from scratch by following the core ideas from the "Attention is All You Need" paper. The model supports training on custom datasets with adjustable configurations for both CPU and GPU environments.

---

## üöÄ Features

- Transformer architecture based on GPT-2
- Causal (autoregressive) self-attention
- Configurable number of layers, heads, embedding size
- Token and positional embeddings
- Supports CPU and GPU training
- Gradient accumulation (for memory efficiency on GPUs)
- Easy to extend for fine-tuning or custom datasets

---

## üß™ CPU Training Configuration

| Hyperparameter       | Value    |
|----------------------|----------|
| `batch_size`         | 12       |
| `block_size`         | 64       |
| `n_embd`             | 128      |
| `n_head`             | 4        |
| `n_layer`            | 4        |
| `dropout`            | 0.0      |
| `learning_rate`      | 3e-4     |
| `max_iterations`     | 4000     |
| **Total Parameters** | ~0.8M    |
| **Runtime**          | ~14 min on CPU |

---

## ‚ö° GPU Training Configuration (e.g. A100, RTX 3090)

| Hyperparameter       | Value    |
|----------------------|----------|
| `batch_size`         | 12       |
| `block_size`         | 1024     |
| `n_embd`             | 768      |
| `n_head`             | 12       |
| `n_layer`            | 12       |
| `learning_rate`      | 6e-4     |
| `max_iterations`     | 600000   |
| **Total Parameters** | ~124M    |
| **Expected Quality** | GPT-2 Small level |

---

## üèóÔ∏è Model Architecture

- **Embedding Layer**: Token + Positional embeddings
- **Stacked Transformer Blocks** (n_layer):
  - LayerNorm
  - Multi-Head Self Attention
  - FeedForward MLP
  - Residual Connections
- **Final LayerNorm**
- **Output Projection**: tied with token embedding (optional)

---
## Generation for example
```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

---

## Note By
I don't have a GPU, so I trained the model with 0.8M parameters only :).
